{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7077e81c",
   "metadata": {},
   "source": [
    "# ğŸ§¹ Template de Limpieza de Datos - Capa Bronce\n",
    "\n",
    "## Objetivo\n",
    "Cargar datos desde MinIO (capa Bronce) y realizar operaciones de limpieza y transformaciÃ³n en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c50c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "import tempfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d31c6c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conectando a MinIO: localhost:9000\n",
      "ğŸ“¦ Bucket: meteo-bronze\n",
      "âœ… ConexiÃ³n a MinIO establecida\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ConfiguraciÃ³n de MinIO\n",
    "# Obtener credenciales desde variables de entorno (O configurarlas directamente aquÃ­)\n",
    "\n",
    "MINIO_ENDPOINT = os.environ.get('MINIO_ENDPOINT', 'localhost:9000')\n",
    "MINIO_ACCESS_KEY = os.environ.get('MINIO_ACCESS_KEY', 'minioadmin')\n",
    "MINIO_SECRET_KEY = os.environ.get('MINIO_SECRET_KEY', 'minioadmin')\n",
    "MINIO_BUCKET = os.environ.get('MINIO_BUCKET', 'meteo-bronze')\n",
    "\n",
    "print(f\"âœ… Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "print(f\"ğŸ“¦ Bucket: {MINIO_BUCKET}\")\n",
    "\n",
    "# Crear cliente MinIO\n",
    "minio_client = Minio(\n",
    "    MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "print(\"âœ… ConexiÃ³n a MinIO establecida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b3fc72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession iniciada\n",
      "   df = PySpark DataFrame\n",
      "   MÃ©todos: .show(), .filter(), .select(), .rename(), etc.\n"
     ]
    }
   ],
   "source": [
    "# âš¡ Inicializar SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LimpiezaDatos\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"âœ… SparkSession iniciada\")\n",
    "print(\"   df = PySpark DataFrame\")\n",
    "print(\"   MÃ©todos: .show(), .filter(), .select(), .rename(), etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6c597a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ Funciones para cargar archivos CSV desde MinIO a PySpark\n",
    "\n",
    "def cargar_csv_desde_minio(nombre_archivo):\n",
    "    \"\"\"\n",
    "    Carga un archivo CSV desde MinIO a un PySpark DataFrame.\n",
    "    \n",
    "    CSV no tiene problemas de nanosegundos (timestamps como strings).\n",
    "    \n",
    "    Args:\n",
    "        nombre_archivo (str): Nombre/ruta del archivo en MinIO\n",
    "        \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame de PySpark con los datos del archivo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸ“¥ Descargando desde MinIO: {nombre_archivo}\")\n",
    "        \n",
    "        # Descargar archivo a archivo temporal\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_file = os.path.join(temp_dir, nombre_archivo.split('/')[-1])\n",
    "        \n",
    "        minio_client.fget_object(MINIO_BUCKET, nombre_archivo, temp_file)\n",
    "        print(f\"   âœ… Descargado a: {temp_file}\")\n",
    "        \n",
    "        # Leer CSV con PySpark (inferir esquema automÃ¡ticamente)\n",
    "        df = spark.read.csv(temp_file, header=True, inferSchema=True)\n",
    "        \n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        print(f\"âœ… Cargado en PySpark: {row_count} filas, {col_count} columnas\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def cargar_csv_reciente(nombre_tabla):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV mÃ¡s reciente de una tabla desde MinIO a PySpark.\n",
    "    \n",
    "    Busca archivos con patrÃ³n: {tabla}_bronce_{timestamp}.csv\n",
    "    \n",
    "    Args:\n",
    "        nombre_tabla (str): Nombre de la tabla\n",
    "        \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame de PySpark con los datos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Listar archivos de la tabla\n",
    "        objects = minio_client.list_objects(MINIO_BUCKET, prefix=nombre_tabla, recursive=True)\n",
    "        \n",
    "        archivos = []\n",
    "        for obj in objects:\n",
    "            if obj.object_name.endswith('.csv') and '_bronce_' in obj.object_name:\n",
    "                archivos.append(obj.object_name)\n",
    "        \n",
    "        if not archivos:\n",
    "            print(f\"âš ï¸  No hay archivos CSV para la tabla: {nombre_tabla}\")\n",
    "            return None\n",
    "        \n",
    "        # Ordenar por nombre (el timestamp estÃ¡ en el nombre) y tomar el mÃ¡s reciente\n",
    "        archivo_reciente = sorted(archivos)[-1]\n",
    "        print(f\"ğŸ“„ Archivo mÃ¡s reciente: {archivo_reciente}\")\n",
    "        \n",
    "        return cargar_csv_desde_minio(archivo_reciente)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "44c329d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Archivo mÃ¡s reciente: sensor_readings/sensor_readings_bronce_20251202112554.csv\n",
      "ğŸ“¥ Descargando desde MinIO: sensor_readings/sensor_readings_bronce_20251202112554.csv\n",
      "   âœ… Descargado a: C:\\Users\\ALUMNO~1\\AppData\\Local\\Temp\\sensor_readings_bronce_20251202112554.csv\n",
      "âœ… Cargado en PySpark: 97 filas, 12 columnas\n",
      "âœ… Cargado en PySpark: 97 filas, 12 columnas\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¥ CARGAR DATOS COMO PYSPARK DATAFRAME DESDE CSV\n",
    "\n",
    "# Cargar el archivo CSV mÃ¡s reciente de sensor_readings\n",
    "df = cargar_csv_reciente(\"sensor_readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa85427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š INFORMACIÃ“N DEL DATAFRAME\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Dimensiones:\n",
      "   Filas: 97\n",
      "   Columnas: 12\n",
      "\n",
      "ğŸ”¤ Esquema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- light: integer (nullable = true)\n",
      " |-- uv_level: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rain_raw: integer (nullable = true)\n",
      " |-- wind_raw: integer (nullable = true)\n",
      " |-- vibration: boolean (nullable = true)\n",
      "\n",
      "\n",
      "ğŸ‘€ Primeras 5 filas:\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” INSPECCIONAR PYSPARK DATAFRAME\n",
    "\n",
    "if df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“Š INFORMACIÃ“N DEL DATAFRAME\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nğŸ“ Dimensiones:\")\n",
    "    print(f\"   Filas: {df.count()}\")\n",
    "    print(f\"   Columnas: {len(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¤ Esquema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\nğŸ‘€ Primeras 5 filas:\")\n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"âš ï¸  No hay datos cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a185d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§¹ Limpieza de Datos con PySpark\n",
    "\n",
    "Usa las celdas siguientes para realizar tu limpieza personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "31a839b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š PYSPARK DATAFRAME COMPLETO\n",
      "================================================================================\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "|  6|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2405|       0|   939.0|       0|       0|     true|\n",
      "|  7|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2411|       0|   939.0|       0|       0|     true|\n",
      "|  8|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2432|       0|   939.0|       0|       0|     true|\n",
      "|  9|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2462|       0|   939.0|       0|       0|     true|\n",
      "| 10|2025-10-23 14:01:...|10.207.51.79|       25.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "| 11|2025-10-23 14:01:...|10.207.51.79|       25.0|    38.0| 5.0| 2455|       0|   939.0|       0|       0|     true|\n",
      "| 12|2025-10-23 14:01:...|10.207.51.79|       25.0|    38.0| 6.0| 2449|       0|   939.0|       0|       0|     true|\n",
      "| 13|2025-10-23 14:01:...|10.207.51.79|       25.0|    37.0| 6.0| 2455|       0|   939.0|       0|       0|     true|\n",
      "| 14|2025-10-23 14:02:...|10.207.51.79|       24.0|    38.0| 5.0| 2438|       0|   939.0|       0|       0|     true|\n",
      "| 15|2025-10-23 14:02:...|10.207.51.79|       24.0|    38.0| 5.0| 2437|       0|   939.0|       0|       0|     true|\n",
      "| 16|2025-10-23 14:02:...|10.207.51.79|       24.0|    38.0| 5.0| 2437|       0|   939.0|       0|       0|     true|\n",
      "| 17|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2436|       0|   939.0|       0|       0|     true|\n",
      "| 18|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2474|       0|   939.0|       0|       0|     true|\n",
      "| 19|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2463|       0|   939.0|       0|       0|     true|\n",
      "| 20|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2469|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š PYSPARK DATAFRAME COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "082bad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "|  6|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2405|       0|   939.0|       0|       0|     true|\n",
      "|  7|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2411|       0|   939.0|       0|       0|     true|\n",
      "|  8|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2432|       0|   939.0|       0|       0|     true|\n",
      "|  9|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2462|       0|   939.0|       0|       0|     true|\n",
      "| 10|2025-10-23 14:01:...|10.207.51.79|       25.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar primeras 10 filas con PySpark\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56374f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 97\n",
      "Columnas: 12\n",
      "\n",
      "Esquema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- light: integer (nullable = true)\n",
      " |-- uv_level: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rain_raw: integer (nullable = true)\n",
      " |-- wind_raw: integer (nullable = true)\n",
      " |-- vibration: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# InformaciÃ³n del PySpark DataFrame\n",
    "print(f\"Filas: {df.count()}\")\n",
    "print(f\"Columnas: {len(df.columns)}\")\n",
    "print(f\"\\nEsquema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce32ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filas en rango vÃ¡lido: 97\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§¹ EJEMPLO 1: Filtrar por rango de valores (PySpark SQL)\n",
    "\n",
    "df_clean = df.filter(\"temperature >= 10 AND temperature <= 40\")\n",
    "print(f\"âœ… Filas en rango vÃ¡lido: {df_clean.count()}\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a0d43f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tipos convertidos:\n",
      "root\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- light: integer (nullable = true)\n",
      " |-- uv_level: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rain_raw: integer (nullable = true)\n",
      " |-- wind_raw: integer (nullable = true)\n",
      " |-- vibration: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§¹ EJEMPLO 2: Convertir tipos de datos (PySpark cast)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_clean = df.select(\n",
    "    col(\"temperature\").cast(\"float\").alias(\"temperature\"),\n",
    "    col(\"humidity\").cast(\"float\").alias(\"humidity\"),\n",
    "    \"*\"\n",
    ")\n",
    "print(f\"âœ… Tipos convertidos:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4358fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filas despuÃ©s de eliminar duplicados: 97\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| 78|2025-10-23 14:09:...|10.207.51.79|       24.0|    38.0| 5.0| 2373|       0|   939.0|       0|       0|     true|\n",
      "| 22|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 6.0| 2452|       0|   939.0|       0|       0|     true|\n",
      "| 86|2025-10-23 14:10:...|10.207.51.79|       24.0|    39.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "| 38|2025-10-23 14:04:...|10.207.51.79|       24.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "| 54|2025-10-23 14:05:...|10.207.51.79|       24.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§¹ EJEMPLO 3: Eliminar duplicados (PySpark distinct)\n",
    "\n",
    "df_clean = df.distinct()\n",
    "print(f\"âœ… Filas despuÃ©s de eliminar duplicados: {df_clean.count()}\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4e1f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Columnas seleccionadas: ['temp', 'humedad', 'timestamp']\n",
      "+----+-------+--------------------+\n",
      "|temp|humedad|           timestamp|\n",
      "+----+-------+--------------------+\n",
      "|25.0|   39.0|2025-10-23 14:00:...|\n",
      "|25.0|   39.0|2025-10-23 14:00:...|\n",
      "|25.0|   40.0|2025-10-23 14:00:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "+----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§¹ EJEMPLO 4: Seleccionar y renombrar columnas (PySpark)\n",
    "\n",
    "df_clean = df.select(\n",
    "    col(\"temperature\").alias(\"temp\"),\n",
    "    col(\"humidity\").alias(\"humedad\"),\n",
    "    \"timestamp\"\n",
    ")\n",
    "print(f\"âœ… Columnas seleccionadas: {df_clean.columns}\")\n",
    "df_clean.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_meteo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
