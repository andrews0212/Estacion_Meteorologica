{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ddf93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:59:54.217919Z",
     "iopub.status.busy": "2025-12-04T12:59:54.217919Z",
     "iopub.status.idle": "2025-12-04T12:59:54.517357Z",
     "shell.execute_reply": "2025-12-04T12:59:54.517357Z"
    },
    "papermill": {
     "duration": 0.304021,
     "end_time": "2025-12-04T12:59:54.518901",
     "exception": false,
     "start_time": "2025-12-04T12:59:54.214880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from minio import Minio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66efefda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:59:54.523940Z",
     "iopub.status.busy": "2025-12-04T12:59:54.523940Z",
     "iopub.status.idle": "2025-12-04T13:00:00.292794Z",
     "shell.execute_reply": "2025-12-04T13:00:00.292794Z"
    },
    "papermill": {
     "duration": 5.772924,
     "end_time": "2025-12-04T13:00:00.293832",
     "exception": false,
     "start_time": "2025-12-04T12:59:54.520908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark iniciado\n"
     ]
    }
   ],
   "source": [
    "# Inicializar SparkSession\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "import time\n",
    "time.sleep(1)\n",
    "conf = SparkConf().setAppName(\"LimpiezaSilver\").setMaster(\"local[*]\").set(\"spark.driver.bindAddress\", \"127.0.0.1\").set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "except:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"‚úÖ Spark iniciado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f072ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:00:00.298965Z",
     "iopub.status.busy": "2025-12-04T13:00:00.298965Z",
     "iopub.status.idle": "2025-12-04T13:00:00.326254Z",
     "shell.execute_reply": "2025-12-04T13:00:00.326254Z"
    },
    "papermill": {
     "duration": 0.031324,
     "end_time": "2025-12-04T13:00:00.327262",
     "exception": false,
     "start_time": "2025-12-04T13:00:00.295938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MinIO conectado\n",
      "‚úÖ Bucket meteo-silver ya existe\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n MinIO\n",
    "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\", \"localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.environ.get(\"MINIO_ACCESS_KEY\", \"minioadmin\")\n",
    "MINIO_SECRET_KEY = os.environ.get(\"MINIO_SECRET_KEY\", \"minioadmin\")\n",
    "MINIO_BUCKET_BRONCE = os.environ.get(\"MINIO_BUCKET\", \"meteo-bronze\")\n",
    "MINIO_BUCKET_SILVER = \"meteo-silver\"\n",
    "\n",
    "minio_client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "print(\"‚úÖ MinIO conectado\")\n",
    "\n",
    "# Crear bucket Silver si no existe\n",
    "try:\n",
    "    minio_client.make_bucket(MINIO_BUCKET_SILVER)\n",
    "    print(f'‚úÖ Bucket {MINIO_BUCKET_SILVER} creado')\n",
    "except:\n",
    "    print(f'‚úÖ Bucket {MINIO_BUCKET_SILVER} ya existe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a53df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:00:00.332294Z",
     "iopub.status.busy": "2025-12-04T13:00:00.331260Z",
     "iopub.status.idle": "2025-12-04T13:00:05.867545Z",
     "shell.execute_reply": "2025-12-04T13:00:05.867040Z"
    },
    "papermill": {
     "duration": 5.53825,
     "end_time": "2025-12-04T13:00:05.867545",
     "exception": false,
     "start_time": "2025-12-04T13:00:00.329295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Buscando archivos Bronce...\n",
      "‚úÖ Cargando: sensor_readings/sensor_readings_bronce_20251204135951.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cargados 6100 registros\n",
      "\n",
      "üìä DataFrame: 6100 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+-----------+--------+-----+-----+--------+--------+--------+--------+---------+\n",
      "|  id|           timestamp|          ip|temperature|humidity| pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+----+--------------------+------------+-----------+--------+-----+-----+--------+--------+--------+--------+---------+\n",
      "|  98| 2024-10-12 14:00:00|192.168.1.50|       18.5|    55.0| 25.0| 7000|       4|  1020.0|       0|      40|     true|\n",
      "|  99| 2024-11-20 11:15:00|192.168.1.50|       10.2|    95.0|  5.0| 1200|       1|   998.0|     450|     120|     true|\n",
      "| 100| 2024-11-30 23:00:00|192.168.1.50|        5.5|    80.0| 18.0|    0|       0|  1018.0|      10|      20|     true|\n",
      "|5705|2024-12-04 14:01:...|192.168.1.50|      16.02|    46.4| 47.0|50074|       3|  1021.0|       0|     245|     true|\n",
      "|4696|2024-12-04 19:04:...|192.168.1.50|      16.34|    47.3| 17.0|30434|       3|  1030.0|       0|     170|     true|\n",
      "|3105|2024-12-04 19:33:...|192.168.1.50|      11.72|    59.6|133.0|27860|       1|  1027.0|       0|     300|     true|\n",
      "|5146|2024-12-04 20:44:...|192.168.1.50|       7.31|    38.1| 84.0|16268|       1|  1020.0|       0|       5|     true|\n",
      "|1186|2024-12-04 21:57:...|192.168.1.50|       12.6|    62.5| 48.0|    0|       0|  1020.0|       0|     264|     true|\n",
      "|3219|2024-12-05 00:49:...|192.168.1.50|      10.69|    48.3|120.0|    0|       0|  1022.0|       0|      85|     true|\n",
      "|5473|2024-12-05 01:53:...|192.168.1.50|      11.91|    60.5|111.0|    0|       0|  1013.0|       0|     172|     true|\n",
      "|1704|2024-12-05 03:59:...|192.168.1.50|       3.31|    61.8| 22.0|    0|       0|  1026.0|       0|     260|     true|\n",
      "|4694|2024-12-05 04:48:...|192.168.1.50|       7.69|    45.3| 93.0|    0|       0|  1016.0|       0|     237|     true|\n",
      "|5271|2024-12-05 04:58:...|192.168.1.50|      -0.66|    75.3| 43.0|    0|       0|  1010.0|       0|     220|     true|\n",
      "|1661|2024-12-05 06:06:...|192.168.1.50|      12.43|    37.1| 59.0|    0|       0|  1017.0|       0|      64|     true|\n",
      "|1909|2024-12-05 06:54:...|192.168.1.50|       7.42|    37.3| 24.0|    0|       0|  1013.0|       0|     163|     true|\n",
      "|4358|2024-12-05 07:49:...|192.168.1.50|       7.49|    72.0| 70.0|    0|       0|  1013.0|       0|     112|     true|\n",
      "|1682|2024-12-05 07:51:...|192.168.1.50|       6.67|    38.7| 59.0|    0|       0|  1019.0|       0|     140|     true|\n",
      "|1250|2024-12-05 09:19:...|192.168.1.50|       5.08|    46.4| 30.0| 8328|       3|  1019.0|       0|     221|     true|\n",
      "|1630|2024-12-05 12:20:...|192.168.1.50|       6.42|    87.3| 28.0| 3753|       1|   996.0|     781|     364|     true|\n",
      "|3616|2024-12-05 13:42:...|192.168.1.50|       6.81|    75.1| 22.0|51477|       3|  1017.0|       0|     258|     true|\n",
      "+----+--------------------+------------+-----------+--------+-----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar archivo m√°s reciente de Bronze\n",
    "archivo_reciente = None\n",
    "try:\n",
    "    print(\"üì• Buscando archivos Bronce...\")\n",
    "    objects = minio_client.list_objects(MINIO_BUCKET_BRONCE, recursive=True)\n",
    "    archivos_csv = [obj.object_name for obj in objects if obj.object_name.endswith(\".csv\")]\n",
    "    if archivos_csv:\n",
    "        archivo_reciente = sorted(archivos_csv)[-1]\n",
    "        print(f\"‚úÖ Cargando: {archivo_reciente}\")\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_file = os.path.join(temp_dir, archivo_reciente.split(\"/\")[-1])\n",
    "        minio_client.fget_object(MINIO_BUCKET_BRONCE, archivo_reciente, temp_file)\n",
    "        df = spark.read.csv(temp_file, header=True, inferSchema=True)\n",
    "        print(f\"‚úÖ Cargados {df.count()} registros\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Sin archivos en Bronce\")\n",
    "        df = spark.createDataFrame([(1, 25.5, 60)], [\"id\", \"temperature\", \"humidity\"])\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    df = spark.createDataFrame([(1, 25.5, 60)], [\"id\", \"temperature\", \"humidity\"])\n",
    "\n",
    "print(f\"\\nüìä DataFrame: {df.count()} registros\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8078c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:00:05.874553Z",
     "iopub.status.busy": "2025-12-04T13:00:05.873556Z",
     "iopub.status.idle": "2025-12-04T13:00:08.133672Z",
     "shell.execute_reply": "2025-12-04T13:00:08.132663Z"
    },
    "papermill": {
     "duration": 2.263121,
     "end_time": "2025-12-04T13:00:08.133672",
     "exception": false,
     "start_time": "2025-12-04T13:00:05.870551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ LIMPIEZA DE DATOS...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 6100 registros limpios\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+-----------+--------+-----+--------------+-------------+-------------+--------------+----------------+-----------------+\n",
      "|  id|           timestamp|          ip|temperature|humidity|light|timestamp_anio|timestamp_mes|timestamp_dia|timestamp_hora|timestamp_minuto|timestamp_segundo|\n",
      "+----+--------------------+------------+-----------+--------+-----+--------------+-------------+-------------+--------------+----------------+-----------------+\n",
      "|5474|2024-12-20 08:09:...|192.168.1.50|       7.38|    95.0| 1072|          2024|           12|           20|             8|               9|               13|\n",
      "|5840|2024-12-30 05:57:...|192.168.1.50|       9.45|    53.9|    0|          2024|           12|           30|             5|              57|               17|\n",
      "|3883|2024-12-30 14:18:...|192.168.1.50|      15.45|    38.3|36018|          2024|           12|           30|            14|              18|               53|\n",
      "+----+--------------------+------------+-----------+--------+-----+--------------+-------------+-------------+--------------+----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de datos\n",
    "print(\"\\nüßπ LIMPIEZA DE DATOS...\")\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "df = df.drop('pressure', 'uv_level', 'pm25', 'rain_raw', 'wind_raw', 'vibration')\n",
    "\n",
    "# Eliminar duplicados\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Descomponer timestamp en columnas separadas\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "# Buscar columnas de tipo timestamp\n",
    "timestamp_cols = [field.name for field in df.schema.fields if \"timestamp\" in field.name.lower()]\n",
    "\n",
    "for ts_col in timestamp_cols:\n",
    "    df = df.withColumn(f\"{ts_col}_anio\", year(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_mes\", month(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_dia\", dayofmonth(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_hora\", hour(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_minuto\", minute(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_segundo\", second(col(ts_col)))\n",
    "\n",
    "print(f\"\\n‚úÖ {df.count()} registros limpios\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028df595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:00:08.142621Z",
     "iopub.status.busy": "2025-12-04T13:00:08.142190Z",
     "iopub.status.idle": "2025-12-04T13:00:10.020077Z",
     "shell.execute_reply": "2025-12-04T13:00:10.020077Z"
    },
    "papermill": {
     "duration": 1.884293,
     "end_time": "2025-12-04T13:00:10.021482",
     "exception": false,
     "start_time": "2025-12-04T13:00:08.137189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ GUARDANDO EN SILVER...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sensor_readings_silver.csv actualizado en Silver\n",
      "\n",
      "======================================================================\n",
      "‚úÖ LIMPIEZA DE SILVER COMPLETADA\n",
      "======================================================================\n",
      "üìç Origen: sensor_readings/sensor_readings_bronce_20251204135951.csv\n",
      "üìç Destino: meteo-silver/sensor_readings/sensor_readings_silver.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Registros: 6100 limpios y sin duplicados\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Guardar en Silver (dentro de carpeta con nombre de tabla)\n",
    "\n",
    "print(\"\\nüíæ GUARDANDO EN SILVER...\")\n",
    "\n",
    "# Extraer nombre de tabla desde archivo_reciente (remover carpeta y sufijo _bronce)\n",
    "if archivo_reciente:\n",
    "    tabla_base = archivo_reciente.split('/')[-1]  # Obtener solo el nombre del archivo\n",
    "    tabla = tabla_base.split('_bronce_')[0]  # Remover sufijo _bronce_\n",
    "else:\n",
    "    tabla = 'datos'\n",
    "\n",
    "archivo_silver = f'{tabla}_silver.csv'\n",
    "\n",
    "try:\n",
    "    import io\n",
    "    from pyspark.sql.types import TimestampType\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # --- SOLUCI√ìN DEL ERROR ---\n",
    "    # Convertimos las columnas de fecha (Timestamp) a String DENTRO de Spark.\n",
    "    # Esto evita que Pandas intente hacer la conversi√≥n estricta de datetime64[ns] que est√° fallando.\n",
    "    df_export = df  # Creamos una copia para exportar\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, TimestampType):\n",
    "            # Forzamos el formato string para el CSV\n",
    "            df_export = df_export.withColumn(field.name, col(field.name).cast(\"string\"))\n",
    "    # --------------------------\n",
    "\n",
    "    # Convertir a pandas y generar CSV en memoria (evita Hadoop)\n",
    "    # Ahora usamos df_export en lugar de df\n",
    "    pdf = df_export.toPandas()\n",
    "    \n",
    "    csv_buffer = pdf.to_csv(index=False)\n",
    "    \n",
    "    # Subir con nombre de tabla espec√≠fico dentro de carpeta (tabla/archivo.csv)\n",
    "    object_path = f\"{tabla}/{archivo_silver}\"\n",
    "    csv_bytes = io.BytesIO(csv_buffer.encode('utf-8'))\n",
    "    minio_client.put_object(\n",
    "        MINIO_BUCKET_SILVER, \n",
    "        object_path, \n",
    "        csv_bytes,\n",
    "        length=len(csv_buffer.encode('utf-8')),\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    print(f\"‚úÖ {archivo_silver} actualizado en Silver\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ LIMPIEZA DE SILVER COMPLETADA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìç Origen: {archivo_reciente}\")\n",
    "    print(f\"üìç Destino: meteo-silver/{object_path}\")\n",
    "    print(f\"üìä Registros: {df.count()} limpios y sin duplicados\")\n",
    "    print(\"=\"*70)\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"UnsatisfiedLinkError\" in error_msg or \"NativeIO\" in error_msg:\n",
    "        print(f\"‚ö†Ô∏è  Warning Hadoop ignorado (no cr√≠tico)\")\n",
    "        print(\"‚úÖ LIMPIEZA DE SILVER COMPLETADA\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_meteo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.16864,
   "end_time": "2025-12-04T13:00:10.476802",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\notebooks\\templates\\limpieza_template.ipynb",
   "output_path": "C:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\notebooks\\templates\\limpieza_template.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T12:59:52.308162",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}