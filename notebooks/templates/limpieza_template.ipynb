{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7077e81c",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Template de Limpieza de Datos - Capa Bronce\n",
    "\n",
    "## Objetivo\n",
    "Cargar datos desde MinIO (capa Bronce) y realizar operaciones de limpieza y transformaciÃ³n en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0c50c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "import tempfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d31c6c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conectando a MinIO: localhost:9000\n",
      "ðŸ“¦ Bucket: meteo-bronze\n",
      "âœ… ConexiÃ³n a MinIO establecida\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ ConfiguraciÃ³n de MinIO\n",
    "# Obtener credenciales desde variables de entorno (O configurarlas directamente aquÃ­)\n",
    "\n",
    "MINIO_ENDPOINT = os.environ.get('MINIO_ENDPOINT', 'localhost:9000')\n",
    "MINIO_ACCESS_KEY = os.environ.get('MINIO_ACCESS_KEY', 'minioadmin')\n",
    "MINIO_SECRET_KEY = os.environ.get('MINIO_SECRET_KEY', 'minioadmin')\n",
    "MINIO_BUCKET = os.environ.get('MINIO_BUCKET', 'meteo-bronze')\n",
    "\n",
    "print(f\"âœ… Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "print(f\"ðŸ“¦ Bucket: {MINIO_BUCKET}\")\n",
    "\n",
    "# Crear cliente MinIO\n",
    "minio_client = Minio(\n",
    "    MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "print(\"âœ… ConexiÃ³n a MinIO establecida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4b3fc72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession iniciada\n",
      "   df = PySpark DataFrame\n",
      "   MÃ©todos: .show(), .filter(), .select(), .rename(), etc.\n"
     ]
    }
   ],
   "source": [
    "# âš¡ Inicializar SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LimpiezaDatos\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"âœ… SparkSession iniciada\")\n",
    "print(\"   df = PySpark DataFrame\")\n",
    "print(\"   MÃ©todos: .show(), .filter(), .select(), .rename(), etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6c597a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ Funciones para cargar archivos CSV desde MinIO a PySpark\n",
    "\n",
    "def cargar_csv_desde_minio(nombre_archivo):\n",
    "    \"\"\"\n",
    "    Carga un archivo CSV desde MinIO a un PySpark DataFrame.\n",
    "    \n",
    "    CSV no tiene problemas de nanosegundos (timestamps como strings).\n",
    "    \n",
    "    Args:\n",
    "        nombre_archivo (str): Nombre/ruta del archivo en MinIO\n",
    "        \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame de PySpark con los datos del archivo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ðŸ“¥ Descargando desde MinIO: {nombre_archivo}\")\n",
    "        \n",
    "        # Descargar archivo a archivo temporal\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_file = os.path.join(temp_dir, nombre_archivo.split('/')[-1])\n",
    "        \n",
    "        minio_client.fget_object(MINIO_BUCKET, nombre_archivo, temp_file)\n",
    "        print(f\"   âœ… Descargado a: {temp_file}\")\n",
    "        \n",
    "        # Leer CSV con PySpark (inferir esquema automÃ¡ticamente)\n",
    "        df = spark.read.csv(temp_file, header=True, inferSchema=True)\n",
    "        \n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        print(f\"âœ… Cargado en PySpark: {row_count} filas, {col_count} columnas\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def cargar_csv_reciente(nombre_tabla):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV mÃ¡s reciente de una tabla desde MinIO a PySpark.\n",
    "    \n",
    "    Busca archivos con patrÃ³n: {tabla}_bronce_{timestamp}.csv\n",
    "    \n",
    "    Args:\n",
    "        nombre_tabla (str): Nombre de la tabla\n",
    "        \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame de PySpark con los datos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Listar archivos de la tabla\n",
    "        objects = minio_client.list_objects(MINIO_BUCKET, prefix=nombre_tabla, recursive=True)\n",
    "        \n",
    "        archivos = []\n",
    "        for obj in objects:\n",
    "            if obj.object_name.endswith('.csv') and '_bronce_' in obj.object_name:\n",
    "                archivos.append(obj.object_name)\n",
    "        \n",
    "        if not archivos:\n",
    "            print(f\"âš ï¸  No hay archivos CSV para la tabla: {nombre_tabla}\")\n",
    "            return None\n",
    "        \n",
    "        # Ordenar por nombre (el timestamp estÃ¡ en el nombre) y tomar el mÃ¡s reciente\n",
    "        archivo_reciente = sorted(archivos)[-1]\n",
    "        print(f\"ðŸ“„ Archivo mÃ¡s reciente: {archivo_reciente}\")\n",
    "        \n",
    "        return cargar_csv_desde_minio(archivo_reciente)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "44c329d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Archivo mÃ¡s reciente: sensor_readings/sensor_readings_bronce_20251202112554.csv\n",
      "ðŸ“¥ Descargando desde MinIO: sensor_readings/sensor_readings_bronce_20251202112554.csv\n",
      "   âœ… Descargado a: C:\\Users\\ALUMNO~1\\AppData\\Local\\Temp\\sensor_readings_bronce_20251202112554.csv\n",
      "âœ… Cargado en PySpark: 97 filas, 12 columnas\n",
      "âœ… Cargado en PySpark: 97 filas, 12 columnas\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¥ CARGAR DATOS COMO PYSPARK DATAFRAME DESDE CSV\n",
    "\n",
    "# Cargar el archivo CSV mÃ¡s reciente de sensor_readings\n",
    "df = cargar_csv_reciente(\"sensor_readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fa85427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š INFORMACIÃ“N DEL DATAFRAME\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Dimensiones:\n",
      "   Filas: 97\n",
      "   Columnas: 12\n",
      "\n",
      "ðŸ”¤ Esquema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- light: integer (nullable = true)\n",
      " |-- uv_level: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rain_raw: integer (nullable = true)\n",
      " |-- wind_raw: integer (nullable = true)\n",
      " |-- vibration: boolean (nullable = true)\n",
      "\n",
      "\n",
      "ðŸ‘€ Primeras 5 filas:\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” INSPECCIONAR PYSPARK DATAFRAME\n",
    "\n",
    "if df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ“Š INFORMACIÃ“N DEL DATAFRAME\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Dimensiones:\")\n",
    "    print(f\"   Filas: {df.count()}\")\n",
    "    print(f\"   Columnas: {len(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¤ Esquema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\nðŸ‘€ Primeras 5 filas:\")\n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"âš ï¸  No hay datos cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a185d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§¹ Limpieza de Datos con PySpark\n",
    "\n",
    "Usa las celdas siguientes para realizar tu limpieza personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "31a839b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š PYSPARK DATAFRAME COMPLETO\n",
      "================================================================================\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "|  6|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2405|       0|   939.0|       0|       0|     true|\n",
      "|  7|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2411|       0|   939.0|       0|       0|     true|\n",
      "|  8|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2432|       0|   939.0|       0|       0|     true|\n",
      "|  9|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2462|       0|   939.0|       0|       0|     true|\n",
      "| 10|2025-10-23 14:01:...|10.207.51.79|       25.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "| 11|2025-10-23 14:01:...|10.207.51.79|       25.0|    38.0| 5.0| 2455|       0|   939.0|       0|       0|     true|\n",
      "| 12|2025-10-23 14:01:...|10.207.51.79|       25.0|    38.0| 6.0| 2449|       0|   939.0|       0|       0|     true|\n",
      "| 13|2025-10-23 14:01:...|10.207.51.79|       25.0|    37.0| 6.0| 2455|       0|   939.0|       0|       0|     true|\n",
      "| 14|2025-10-23 14:02:...|10.207.51.79|       24.0|    38.0| 5.0| 2438|       0|   939.0|       0|       0|     true|\n",
      "| 15|2025-10-23 14:02:...|10.207.51.79|       24.0|    38.0| 5.0| 2437|       0|   939.0|       0|       0|     true|\n",
      "| 16|2025-10-23 14:02:...|10.207.51.79|       24.0|    38.0| 5.0| 2437|       0|   939.0|       0|       0|     true|\n",
      "| 17|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2436|       0|   939.0|       0|       0|     true|\n",
      "| 18|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2474|       0|   939.0|       0|       0|     true|\n",
      "| 19|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2463|       0|   939.0|       0|       0|     true|\n",
      "| 20|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 5.0| 2469|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“Š PYSPARK DATAFRAME COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "082bad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "|  6|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2405|       0|   939.0|       0|       0|     true|\n",
      "|  7|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2411|       0|   939.0|       0|       0|     true|\n",
      "|  8|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2432|       0|   939.0|       0|       0|     true|\n",
      "|  9|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2462|       0|   939.0|       0|       0|     true|\n",
      "| 10|2025-10-23 14:01:...|10.207.51.79|       25.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar primeras 10 filas con PySpark\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "56374f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 97\n",
      "Columnas: 12\n",
      "\n",
      "Esquema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- light: integer (nullable = true)\n",
      " |-- uv_level: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rain_raw: integer (nullable = true)\n",
      " |-- wind_raw: integer (nullable = true)\n",
      " |-- vibration: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# InformaciÃ³n del PySpark DataFrame\n",
    "print(f\"Filas: {df.count()}\")\n",
    "print(f\"Columnas: {len(df.columns)}\")\n",
    "print(f\"\\nEsquema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ce32ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filas en rango vÃ¡lido: 97\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "|  1|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2426|       0|   939.0|       0|       0|     true|\n",
      "|  2|2025-10-23 14:00:...|10.207.51.79|       25.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "|  3|2025-10-23 14:00:...|10.207.51.79|       25.0|    40.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "|  4|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2382|       0|   939.0|       0|       0|     true|\n",
      "|  5|2025-10-23 14:01:...|10.207.51.79|       25.0|    40.0| 5.0| 2410|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¹ EJEMPLO 1: Filtrar por rango de valores (PySpark SQL)\n",
    "\n",
    "df_clean = df.filter(\"temperature >= 10 AND temperature <= 40\")\n",
    "print(f\"âœ… Filas en rango vÃ¡lido: {df_clean.count()}\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a0d43f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tipos convertidos:\n",
      "root\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- light: integer (nullable = true)\n",
      " |-- uv_level: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rain_raw: integer (nullable = true)\n",
      " |-- wind_raw: integer (nullable = true)\n",
      " |-- vibration: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¹ EJEMPLO 2: Convertir tipos de datos (PySpark cast)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_clean = df.select(\n",
    "    col(\"temperature\").cast(\"float\").alias(\"temperature\"),\n",
    "    col(\"humidity\").cast(\"float\").alias(\"humidity\"),\n",
    "    \"*\"\n",
    ")\n",
    "print(f\"âœ… Tipos convertidos:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4776104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 97\n"
     ]
    }
   ],
   "source": [
    "# .count() devuelve el nÃºmero total de filas del DataFrame\n",
    "row_count = df.count()\n",
    "print(f'Number of rows: {row_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4358fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filas despuÃ©s de eliminar duplicados: 97\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| 78|2025-10-23 14:09:...|10.207.51.79|       24.0|    38.0| 5.0| 2373|       0|   939.0|       0|       0|     true|\n",
      "| 22|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 6.0| 2452|       0|   939.0|       0|       0|     true|\n",
      "| 86|2025-10-23 14:10:...|10.207.51.79|       24.0|    39.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "| 38|2025-10-23 14:04:...|10.207.51.79|       24.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "| 54|2025-10-23 14:05:...|10.207.51.79|       24.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| id|           timestamp|          ip|temperature|humidity|pm25|light|uv_level|pressure|rain_raw|wind_raw|vibration|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "| 78|2025-10-23 14:09:...|10.207.51.79|       24.0|    38.0| 5.0| 2373|       0|   939.0|       0|       0|     true|\n",
      "| 22|2025-10-23 14:02:...|10.207.51.79|       24.0|    39.0| 6.0| 2452|       0|   939.0|       0|       0|     true|\n",
      "| 86|2025-10-23 14:10:...|10.207.51.79|       24.0|    39.0| 5.0| 2371|       0|   939.0|       0|       0|     true|\n",
      "| 38|2025-10-23 14:04:...|10.207.51.79|       24.0|    39.0| 5.0| 2422|       0|   939.0|       0|       0|     true|\n",
      "| 54|2025-10-23 14:05:...|10.207.51.79|       24.0|    39.0| 5.0| 2466|       0|   939.0|       0|       0|     true|\n",
      "+---+--------------------+------------+-----------+--------+----+-----+--------+--------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¹ EJEMPLO 3: Eliminar duplicados (PySpark distinct)\n",
    "\n",
    "df_clean = df.distinct()\n",
    "print(f\"âœ… Filas despuÃ©s de eliminar duplicados: {df_clean.count()}\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b4e1f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Columnas seleccionadas: ['temp', 'humedad', 'timestamp']\n",
      "+----+-------+--------------------+\n",
      "|temp|humedad|           timestamp|\n",
      "+----+-------+--------------------+\n",
      "|25.0|   39.0|2025-10-23 14:00:...|\n",
      "|25.0|   39.0|2025-10-23 14:00:...|\n",
      "|25.0|   40.0|2025-10-23 14:00:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "+----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¹ EJEMPLO 4: Seleccionar y renombrar columnas (PySpark)\n",
    "\n",
    "df_clean = df.select(\n",
    "    col(\"temperature\").alias(\"temp\"),\n",
    "    col(\"humidity\").alias(\"humedad\"),\n",
    "    \"timestamp\"\n",
    ")\n",
    "print(f\"âœ… Columnas seleccionadas: {df_clean.columns}\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f449ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+\n",
      "|temp|humedad|           timestamp|\n",
      "+----+-------+--------------------+\n",
      "|25.0|   39.0|2025-10-23 14:00:...|\n",
      "|25.0|   39.0|2025-10-23 14:00:...|\n",
      "|25.0|   40.0|2025-10-23 14:00:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   40.0|2025-10-23 14:01:...|\n",
      "|25.0|   39.0|2025-10-23 14:01:...|\n",
      "|25.0|   38.0|2025-10-23 14:01:...|\n",
      "|25.0|   38.0|2025-10-23 14:01:...|\n",
      "|25.0|   37.0|2025-10-23 14:01:...|\n",
      "|24.0|   38.0|2025-10-23 14:02:...|\n",
      "|24.0|   38.0|2025-10-23 14:02:...|\n",
      "|24.0|   38.0|2025-10-23 14:02:...|\n",
      "|24.0|   39.0|2025-10-23 14:02:...|\n",
      "|24.0|   39.0|2025-10-23 14:02:...|\n",
      "|24.0|   39.0|2025-10-23 14:02:...|\n",
      "|24.0|   39.0|2025-10-23 14:02:...|\n",
      "+----+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.drop(\"uv_level\")\n",
    "df_clean = df_clean.drop(\"vibration\")\n",
    "df_clean = df_clean.drop(\"rain_raw\")\n",
    "df_clean = df_clean.drop(\"wind_raw\")\n",
    "df_clean = df_clean.drop(\"pressure\")\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dab51a",
   "metadata": {},
   "source": [
    "1. Significado de las Columnas\n",
    "id: Es un identificador Ãºnico para cada registro.\n",
    "\n",
    "ObservaciÃ³n: FÃ­jate que no estÃ¡n en orden (78, 22, 86...). Esto es normal en Spark porque procesa los datos en paralelo y no garantiza el orden visual a menos que lo ordenes explÃ­citamente (orderBy).\n",
    "\n",
    "timestamp: La fecha y hora exacta en que se tomÃ³ la lectura.\n",
    "\n",
    "Dato: Las lecturas son del 23 de Octubre de 2025, entre las 14:00 y 14:10 horas.\n",
    "\n",
    "ip: La direcciÃ³n IP del dispositivo que enviÃ³ los datos (10.207.51.79).\n",
    "\n",
    "Dato: Al ser la misma en todas las filas, indica que todos estos datos provienen de una sola estaciÃ³n o sensor.\n",
    "\n",
    "temperature: La temperatura ambiente en grados Celsius (probablemente).\n",
    "\n",
    "Valor: Oscila entre 24.0Â°C y 25.0Â°C.\n",
    "\n",
    "humidity: La Humedad Relativa en porcentaje (%).\n",
    "\n",
    "Valor: Oscila entre 38% y 40%. Es un ambiente seco/confortable.\n",
    "\n",
    "pm25: PartÃ­culas por MillÃ³n de 2.5 micras (PM2.5). Es un indicador de calidad del aire.\n",
    "\n",
    "Valor: 5.0 - 6.0. Son valores bajos, lo que indica una buena calidad del aire.\n",
    "\n",
    "light: Nivel de luminosidad o intensidad de luz (probablemente en Lux o un valor crudo del sensor analÃ³gico).\n",
    "\n",
    "Valor: Alrededor de 2400. Indica que es de dÃ­a o hay luz artificial fuerte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸ† CAPA SILVER: Guardar Datos Limpios en MinIO\n",
    "\n",
    "Una vez que hayas limpiado los datos, puedes guardarlos en la capa Silver de MinIO.\n",
    "\n",
    "La capa Silver almacena datos procesados y limpios en formato Parquet.\n",
    "\n",
    "**Buckets:**\n",
    "- **Bronce** (`meteo-bronze`): Datos crudos del ETL (CSV)\n",
    "- **Silver** (`meteo-silver`): Datos limpios y transformados (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d0d9d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Bucket Silver creado: meteo-silver\n",
      "\n",
      "[INFO] Guardando datos limpios en Parquet...\n",
      "   Archivo: sensor_readings_silver_20251202125301.parquet\n",
      "   Filas: 84\n",
      "[ERROR] Error guardando en Silver: An error occurred while calling o446.parquet.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "[ERROR] Error guardando en Silver: An error occurred while calling o446.parquet.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alumno_AI\\AppData\\Local\\Temp\\ipykernel_15916\\217100020.py\", line 31, in <module>\n",
      "    df_sin_outliers_temp.write.mode(\"overwrite\").parquet(temp_file)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\venv_meteo\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 1721, in parquet\n",
      "  File \"c:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\venv_meteo\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    args_command = \"\".join(\n",
      "        [get_command_part(arg, self.pool) for arg in new_args])\n",
      "  File \"c:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\venv_meteo\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "  File \"c:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\venv_meteo\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    if answer[1] == REFERENCE_TYPE:\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        raise Py4JJavaError(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "            \"An error occurred while calling {0}{1}{2}.\\n\".\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o446.parquet.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’¾ GUARDAR DATOS LIMPIOS EN CAPA SILVER (MinIO)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear bucket Silver si no existe\n",
    "bucket_silver = MINIO_BUCKET.replace(\"-bronze\", \"-silver\")\n",
    "\n",
    "try:\n",
    "    if not minio_client.bucket_exists(bucket_silver):\n",
    "        minio_client.make_bucket(bucket_silver)\n",
    "        print(f\"[OK] Bucket Silver creado: {bucket_silver}\")\n",
    "    else:\n",
    "        print(f\"[OK] Bucket Silver existe: {bucket_silver}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error creando bucket: {e}\")\n",
    "\n",
    "# Guardar el DataFrame limpiado en formato Parquet\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "archivo_silver = f\"sensor_readings_silver_{timestamp}.parquet\"\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "temp_file = os.path.join(temp_dir, archivo_silver)\n",
    "\n",
    "print(f\"\\n[INFO] Guardando datos limpios en Parquet...\")\n",
    "print(f\"   Archivo: {archivo_silver}\")\n",
    "print(f\"   Filas: {df_sin_outliers_temp.count()}\")\n",
    "\n",
    "try:\n",
    "    # Guardar como Parquet\n",
    "    df_sin_outliers_temp.write.mode(\"overwrite\").parquet(temp_file)\n",
    "    \n",
    "    # Subir a MinIO\n",
    "    minio_client.fput_object(bucket_silver, archivo_silver, temp_file)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(temp_file) / (1024 * 1024)\n",
    "    print(f\"[OK] Archivo guardado en {bucket_silver}\")\n",
    "    print(f\"   TamaÃ±o: {file_size_mb:.2f} MB\")\n",
    "    print(f\"   Ruta: {bucket_silver}/{archivo_silver}\")\n",
    "    \n",
    "    # Limpiar temporal\n",
    "    try:\n",
    "        os.remove(temp_file)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error guardando en Silver: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_meteo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
