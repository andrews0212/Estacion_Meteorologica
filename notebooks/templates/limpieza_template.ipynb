{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ddf93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:43:41.602073Z",
     "iopub.status.busy": "2025-12-04T12:43:41.601041Z",
     "iopub.status.idle": "2025-12-04T12:43:41.873356Z",
     "shell.execute_reply": "2025-12-04T12:43:41.873356Z"
    },
    "papermill": {
     "duration": 0.279508,
     "end_time": "2025-12-04T12:43:41.874393",
     "exception": false,
     "start_time": "2025-12-04T12:43:41.594885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from minio import Minio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66efefda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:43:41.878970Z",
     "iopub.status.busy": "2025-12-04T12:43:41.878970Z",
     "iopub.status.idle": "2025-12-04T12:43:47.102737Z"
    },
    "papermill": {
     "duration": 4.042976,
     "end_time": "2025-12-04T12:43:45.919369",
     "exception": false,
     "start_time": "2025-12-04T12:43:41.876393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m conf = SparkConf().setAppName(\u001b[33m\"\u001b[39m\u001b[33mLimpiezaSilver\u001b[39m\u001b[33m\"\u001b[39m).setMaster(\u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m).set(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.bindAddress\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m\"\u001b[39m).set(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.host\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Estacion_Meteorologica\\venv_meteo\\Lib\\site-packages\\pyspark\\context.py:377\u001b[39m, in \u001b[36mSparkContext._do_init.<locals>.signal_handler\u001b[39m\u001b[34m(signal, frame)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msignal_handler\u001b[39m(signal: Any, frame: Any) -> NoReturn:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcancelAllJobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Estacion_Meteorologica\\venv_meteo\\Lib\\site-packages\\pyspark\\context.py:2255\u001b[39m, in \u001b[36mSparkContext.cancelAllJobs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcancelAllJobs\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2245\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2246\u001b[39m \u001b[33;03m    Cancel all jobs that have been scheduled or are running.\u001b[39;00m\n\u001b[32m   2247\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2253\u001b[39m \u001b[33;03m    :meth:`SparkContext.runJob`\u001b[39;00m\n\u001b[32m   2254\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2255\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m().cancelAllJobs()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "# Inicializar SparkSession\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "import time\n",
    "time.sleep(1)\n",
    "conf = SparkConf().setAppName(\"LimpiezaSilver\").setMaster(\"local[*]\").set(\"spark.driver.bindAddress\", \"127.0.0.1\").set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "except:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"‚úÖ Spark iniciado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f072ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:42:36.415840Z",
     "iopub.status.busy": "2025-12-04T12:42:36.415840Z",
     "iopub.status.idle": "2025-12-04T12:42:36.458100Z",
     "shell.execute_reply": "2025-12-04T12:42:36.456084Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n MinIO\n",
    "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\", \"localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.environ.get(\"MINIO_ACCESS_KEY\", \"minioadmin\")\n",
    "MINIO_SECRET_KEY = os.environ.get(\"MINIO_SECRET_KEY\", \"minioadmin\")\n",
    "MINIO_BUCKET_BRONCE = os.environ.get(\"MINIO_BUCKET\", \"meteo-bronze\")\n",
    "MINIO_BUCKET_SILVER = \"meteo-silver\"\n",
    "\n",
    "minio_client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "print(\"‚úÖ MinIO conectado\")\n",
    "\n",
    "# Crear bucket Silver si no existe\n",
    "try:\n",
    "    minio_client.make_bucket(MINIO_BUCKET_SILVER)\n",
    "    print(f'‚úÖ Bucket {MINIO_BUCKET_SILVER} creado')\n",
    "except:\n",
    "    print(f'‚úÖ Bucket {MINIO_BUCKET_SILVER} ya existe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a53df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:42:36.470116Z",
     "iopub.status.busy": "2025-12-04T12:42:36.470116Z",
     "iopub.status.idle": "2025-12-04T12:42:51.426172Z",
     "shell.execute_reply": "2025-12-04T12:42:51.425164Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargar archivo m√°s reciente de Bronze\n",
    "archivo_reciente = None\n",
    "try:\n",
    "    print(\"üì• Buscando archivos Bronce...\")\n",
    "    objects = minio_client.list_objects(MINIO_BUCKET_BRONCE, recursive=True)\n",
    "    archivos_csv = [obj.object_name for obj in objects if obj.object_name.endswith(\".csv\")]\n",
    "    if archivos_csv:\n",
    "        archivo_reciente = sorted(archivos_csv)[-1]\n",
    "        print(f\"‚úÖ Cargando: {archivo_reciente}\")\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_file = os.path.join(temp_dir, archivo_reciente.split(\"/\")[-1])\n",
    "        minio_client.fget_object(MINIO_BUCKET_BRONCE, archivo_reciente, temp_file)\n",
    "        df = spark.read.csv(temp_file, header=True, inferSchema=True)\n",
    "        print(f\"‚úÖ Cargados {df.count()} registros\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Sin archivos en Bronce\")\n",
    "        df = spark.createDataFrame([(1, 25.5, 60)], [\"id\", \"temperature\", \"humidity\"])\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    df = spark.createDataFrame([(1, 25.5, 60)], [\"id\", \"temperature\", \"humidity\"])\n",
    "\n",
    "print(f\"\\nüìä DataFrame: {df.count()} registros\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8078c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:05:05.011984Z",
     "iopub.status.busy": "2025-12-04T12:05:05.011984Z",
     "iopub.status.idle": "2025-12-04T12:05:06.811097Z",
     "shell.execute_reply": "2025-12-04T12:05:06.810088Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limpieza de datos\n",
    "print(\"\\nüßπ LIMPIEZA DE DATOS...\")\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "df = df.drop('pressure', 'uv_level', 'pm25', 'rain_raw', 'wind_raw', 'vibration')\n",
    "\n",
    "# Eliminar duplicados\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Descomponer timestamp en columnas separadas\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "# Buscar columnas de tipo timestamp\n",
    "timestamp_cols = [field.name for field in df.schema.fields if \"timestamp\" in field.name.lower()]\n",
    "\n",
    "for ts_col in timestamp_cols:\n",
    "    df = df.withColumn(f\"{ts_col}_anio\", year(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_mes\", month(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_dia\", dayofmonth(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_hora\", hour(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_minuto\", minute(col(ts_col))) \\\n",
    "            .withColumn(f\"{ts_col}_segundo\", second(col(ts_col)))\n",
    "\n",
    "print(f\"\\n‚úÖ {df.count()} registros limpios\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028df595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:05:06.817605Z",
     "iopub.status.busy": "2025-12-04T12:05:06.817605Z",
     "iopub.status.idle": "2025-12-04T12:05:08.639826Z",
     "shell.execute_reply": "2025-12-04T12:05:08.638819Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardar en Silver (2 archivos: nombre tabla + nombre est√°ndar para Power BI)\n",
    "\n",
    "print(\"\\nüíæ GUARDANDO EN SILVER...\")\n",
    "\n",
    "tabla = archivo_reciente.split('_bronce_')[0] if archivo_reciente and '_bronce_' in archivo_reciente else 'datos'\n",
    "archivo_silver = f'{tabla}_silver.csv'\n",
    "archivo_standard = 'datos_principales_silver.csv'  # Nombre est√°ndar para Power BI\n",
    "\n",
    "try:\n",
    "    import io\n",
    "    from pyspark.sql.types import TimestampType\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # --- SOLUCI√ìN DEL ERROR ---\n",
    "    # Convertimos las columnas de fecha (Timestamp) a String DENTRO de Spark.\n",
    "    # Esto evita que Pandas intente hacer la conversi√≥n estricta de datetime64[ns] que est√° fallando.\n",
    "    df_export = df  # Creamos una copia para exportar\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, TimestampType):\n",
    "            # Forzamos el formato string para el CSV\n",
    "            df_export = df_export.withColumn(field.name, col(field.name).cast(\"string\"))\n",
    "    # --------------------------\n",
    "\n",
    "    # Convertir a pandas y generar CSV en memoria (evita Hadoop)\n",
    "    # Ahora usamos df_export en lugar de df\n",
    "    pdf = df_export.toPandas()\n",
    "    \n",
    "    csv_buffer = pdf.to_csv(index=False)\n",
    "    csv_bytes = io.BytesIO(csv_buffer.encode('utf-8'))\n",
    "    \n",
    "    # Subir con nombre de tabla espec√≠fico\n",
    "    csv_bytes_1 = io.BytesIO(csv_buffer.encode('utf-8'))\n",
    "    minio_client.put_object(\n",
    "        MINIO_BUCKET_SILVER, \n",
    "        archivo_silver, \n",
    "        csv_bytes_1,\n",
    "        length=len(csv_buffer.encode('utf-8')),\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    print(f\"‚úÖ {archivo_silver} actualizado en Silver\")\n",
    "    \n",
    "    # Subir con nombre est√°ndar para Power BI (meteo-silver/datos_principales_silver.csv)\n",
    "    csv_bytes_2 = io.BytesIO(csv_buffer.encode('utf-8'))\n",
    "    minio_client.put_object(\n",
    "        MINIO_BUCKET_SILVER, \n",
    "        archivo_standard, \n",
    "        csv_bytes_2,\n",
    "        length=len(csv_buffer.encode('utf-8')),\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    print(f\"‚úÖ {archivo_standard} actualizado en Silver\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ LIMPIEZA DE SILVER COMPLETADA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìç Origen: {archivo_reciente}\")\n",
    "    print(f\"üìç Destino (tabla): meteo-silver/{archivo_silver}\")\n",
    "    print(f\"üìç Destino (Power BI): meteo-silver/{archivo_standard}\")\n",
    "    print(f\"üìä Registros: {df.count()} limpios y sin duplicados\")\n",
    "    print(\"=\"*70)\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"UnsatisfiedLinkError\" in error_msg or \"NativeIO\" in error_msg:\n",
    "        print(f\"‚ö†Ô∏è  Warning Hadoop ignorado (no cr√≠tico)\")\n",
    "        print(\"‚úÖ LIMPIEZA DE SILVER COMPLETADA\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_meteo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.558046,
   "end_time": "2025-12-04T12:43:47.357714",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\notebooks\\templates\\limpieza_template.ipynb",
   "output_path": "C:\\Users\\Alumno_AI\\Desktop\\Estacion_Meteorologica\\notebooks\\templates\\limpieza_template.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T12:43:39.799668",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}